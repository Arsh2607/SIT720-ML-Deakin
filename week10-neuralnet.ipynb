{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:21:42.665644Z","iopub.status.busy":"2023-05-17T12:21:42.664988Z","iopub.status.idle":"2023-05-17T12:21:52.301085Z","shell.execute_reply":"2023-05-17T12:21:52.299956Z","shell.execute_reply.started":"2023-05-17T12:21:42.665585Z"},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from torch import nn\n","from torch.autograd import Variable \n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.datasets import fetch_lfw_people\n","from sklearn.model_selection import train_test_split\n","import torch\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.neural_network import MLPClassifier"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:56:13.094421Z","iopub.status.busy":"2023-05-17T12:56:13.094000Z","iopub.status.idle":"2023-05-17T12:56:13.284218Z","shell.execute_reply":"2023-05-17T12:56:13.283216Z","shell.execute_reply.started":"2023-05-17T12:56:13.094387Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape:  (1140, 2914)\n","y shape:  (1140,)\n","target_names:  ['Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder'\n"," 'Tony Blair']\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(array([0, 1, 2, 3, 4]), array([175,  85, 408,  78, 109]))\n","(array([0, 1, 2, 3, 4]), array([ 61,  36, 122,  31,  35]))\n"]}],"source":["# Load data\n","lfw_dataset = fetch_lfw_people(min_faces_per_person=100)\n","_, h, w = lfw_dataset.images.shape\n","X = lfw_dataset.data\n","y = lfw_dataset.target\n","target_names = lfw_dataset.target_names\n","n_samples, h, w = lfw_dataset.images.shape\n","print(\"X shape: \", X.shape)\n","print(\"y shape: \", y.shape)\n","print(\"target_names: \", target_names)\n","plt.imshow(X[0].reshape((h, w)), cmap=plt.cm.gray)\n","plt.title(target_names[1], size=12)\n","plt.xticks(())\n","plt.yticks(())\n","plt.show()\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","# count unique values of y_train\n","print(np.unique(y_train, return_counts=True))\n","print(np.unique(y_test, return_counts=True))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1. What is a neural network? \n","\n","### 1.1 Intuitive Understanding of Neural Networks\n","* NN is analogous to the neural system of brains (**neural diagram** can be seen in the left of the figure below)\n","    + Brains rely on >100 billions neurons\n","    + Why is the model of brains so important for intelligence?  1) communicate with each other via synapses. Each neuron can rececive up to 10K connections from/to other neurons; 2) \"Hundreds of different organs, [each carry out their own specific functions](https://human-memory.net/occipital-brain-lobe/)\".  \n","    * Activation process: the process by which a neuron becomes active through connections with other neurons, which then stimulates (triggers the activation of) other connected neurons. \n","    + How to represent the neurons, their connections (synapses) and functional organs mathematically?\n","    <!-- input/intermediate output values; Weights or paramters     -->\n","    <!-- Each layer consists of **input/intermediate/output values (vertices/nodes/neurons)** connected by **weights** (synapses).  -->\n","    + How does the activation process of a neural network stimulate different functions?\n","    <!-- forward pass contains different activation functoins and architectures -->"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.2 Mathematicaly, as shown on the right of the above image,\n","\n","<center><img src=\"https://i.loli.net/2021/08/12/7yaGWTIKlHNz6mD.png\" width='500' ></center>\n","\n","* What are the differences between linear regression, logistic regression and neural network?\n","    + Logistic regression has no intermediate layer, while NN has multiple intermediate layers, each with multiple neurons. \n","    + i.e., a multivirate function with multiple inputs and one output vs a chain of multivariate functions with multiple inputs and multiple outputs\n","    \n","* Linear Layer: a linear transformation/function\n","    + Notation: Let  $m$ = the number of examples, $n_i$ = the input dimension, $n_o$ = the output dimension\n","    + The linear function:  map input $X$ to the output $y$ by finding the parameters $W$ (determined by loss function): \n","    +  $\\hat{y} = W \\cdot X$  where $\\mathbf{X} \\in \\mathbb{R}^{n_i \\times m}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_o \\times n_i}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{n_o \\times m}$\n","<!--  +  The form conforms to the implementation in Tensorflow： $\\hat{y} = X \\cdot W$  where $\\mathbf{X} \\in \\mathbb{R}^{m \\times n_i}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_i \\times n_o}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{m \\times n_o}$ \n","\n","    + Example of three neurons in a 2x3 linear layer where $n_i=2$ and $n_o=3$\n","$$\\text{Neuron}_1 = 2 x_{1}- x_{2}$$\n","$$\\text{Neuron}_2 = -x_{1} + 2x_{2}$$\n","$$\\text{Neuron}_3 = 5x_{1} + 0.5x_{2}$$\n","The output of this layer can be efficiectly calculated via **matrix-vector product**:\n","$$\n","\\left[\\begin{array}{rr}\n","2 & -1 \\\\\n","-1 & 2 \\\\\n","5  & 0.5\n","\\end{array}\\right]\\left[\\begin{array}{l}\n","x_1 \\\\\n","x_2 \n","\\end{array}\\right]\n","$$\n","    + All the connections between two adjacent layers is encoded in the matrix $W=\\left[\\begin{array}{rr}\n","2 & -1 \\\\\n","-1 & 2 \\\\\n","5  & 0.5\n","\\end{array}\\right]$, which should be learned from training data.\n","    + we can efficiently calculate the values for $m$ examples via matrix multipication\n","    $$\n","    \\left[\\begin{array}{rr}\n","    2 & -1 \\\\\n","    -1 & 2 \\\\\n","    5  & 0.5\n","    \\end{array}\\right]\\left[\\begin{array}{l}\n","    x_1^1 & x_1^2& ... & x_1^m\\\\\n","    x_2^1 & x_2^2& ... & x_2^m\n","    \\end{array}\\right]\n","    $$\n","    \n","    \n","    \n","\n","<!--     + If a bias term $b\\in \\mathbb{R}^{n_o \\times 1}$ is added: $\\hat{y} = W \\cdot X + b$. But no need to do that since (1) $W$ do the job of transforming input or $b$ could be considered as  $w_{n+1} \\dot x$ where $x=1$.\n","    + Although the first notation makes more sense for me to imagine it as the transformation in the high-dimension space, the second notation is commonly used for implementation. Here I change (use $\\theta$ and transpose) the second notation into $\\theta^{T} X$. Because $\\theta$ which can avoid confusion for NLP notations and unify weights and bias.\n","    + Anyway, the linear node and bias nodes would not be shown in neural network diagrams since every neuron is assumed to have a linear node along with its corresponding bias. But we just use its output denoted as $z$ where $z = \\theta^{T} X$ -->\n","\n","<!-- * Function v.s. Equation: \n","    + each equation represents a line in two dimensional space (or a plane) \n","    + each function is an affine transformation (linear transformation with bias) which transform the whole plane e.g., $\\text{hidden_neuron_1} = 2 w_{1}- w_{2}$\n","\n","$$2 x_{1}- x_{2}=0$$\n","$$-x_{1} + 2x_{2}=3$$\n","This can be also represented in the matrix form to be solved efficiently.\n","$$\n","\\left[\\begin{array}{rr}\n","2 & -1 \\\\\n","-1 & 2\n","\\end{array}\\right]\\left[\\begin{array}{l}\n","x_1 \\\\\n","x_2 \n","\\end{array}\\right]=\\left[\\begin{array}{l}\n","0 \\\\\n","3\n","\\end{array}\\right]\n","$$ -->"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:36:56.845559Z","iopub.status.busy":"2023-05-17T12:36:56.845133Z","iopub.status.idle":"2023-05-17T12:36:56.871924Z","shell.execute_reply":"2023-05-17T12:36:56.870970Z","shell.execute_reply.started":"2023-05-17T12:36:56.845525Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(256,)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# matrix-vector product\n","n_i = X_train.shape[1]\n","n_m = 256\n","n_o = 1\n","\n","# n_i neurons in the input/1st layer\n","x = X_train[0]\n","\n","# activate n_m neurons in the 2nd layer\n","W1 = np.random.uniform(-1/np.sqrt(n_i), 1/np.sqrt(n_i), (n_m, n_i))\n","A1 = np.dot(W1, x)\n","\n","# activate n_o neuron in the output layer\n","W2 = np.random.uniform(-1/np.sqrt(n_m), 1/np.sqrt(n_m), (n_o, n_m))\n","A2 = np.dot(W1, x)\n","A2.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Activation functions\n","Add non-linearity into the neural network\n","* Sigmoid\n","    \n","    $$p =\\frac{1}{1+e^{-x}}$$\n","    + Note that the sigmoid function is also used for calculating probability in the last layer for computing the loss\n","* tanh\n","    $$p=\\frac{2}{1+e^{-2 x}}-1$$"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:14:37.921853Z","iopub.status.busy":"2023-05-17T11:14:37.921447Z","iopub.status.idle":"2023-05-17T11:14:38.124892Z","shell.execute_reply":"2023-05-17T11:14:38.124048Z","shell.execute_reply.started":"2023-05-17T11:14:37.921805Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Sigmoid Function\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x)) \n","\n","# plot\n","z = np.arange(-10, 10, 0.1)\n","s = sigmoid(z)\n","plt.plot(z, s)\n","plt.show()\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:41:02.901477Z","iopub.status.busy":"2023-05-17T12:41:02.901060Z","iopub.status.idle":"2023-05-17T12:41:02.944297Z","shell.execute_reply":"2023-05-17T12:41:02.942551Z","shell.execute_reply.started":"2023-05-17T12:41:02.901442Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'sigmoid' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-50fd4f5d7810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# activate n_o neuron in the output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_prod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"]}],"source":["# activate n_m neurons in the 2nd layer\n","W1 = np.random.uniform(-1/np.sqrt(n_i), 1/np.sqrt(n_i), (n_m, n_i))\n","A1 = np.tanh(np.dot(W1, x))\n","\n","# activate n_o neuron in the output layer\n","W2 = np.random.uniform(-1/np.sqrt(n_m), 1/np.sqrt(n_m), (n_o, n_m))\n","y_prod = sigmoid(np.dot(W2, A1))\n","y_prod"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. More complication architectures for data transformation： convolution (CNN), recurrent (RNN), Transformer\n","\n","* Real-world data often contains properties that are sequential or spatial in nature, such as text, audio, video, sensor readings, and images. These properties can be important in determining the meaning of the data, and therefore need to be taken into account when processing the data.\n","\n","* Sequential properties refer to the fact that the positions of data points matter in explaining the overall meaning of the data, while spatial properties refer to the fact that the relative positions of data points matter in determining the meaning of the data. \n","* Communication using language: we use a sequence of words to express our thoughts and ideas. \n","    + The order in which we arrange these words is very important: \"The cat sat on the mat.\" v.s. \"The mat sat on the cat.\"\n","    + In order to accurately predict or classify language data, a neural network must be able to understand the relationships between words in a sentence and how they are ordered. Hence, sequential neural network architectures are designed to take into account the sequential nature of language data and can effectively learn the complex relationships between words in a sentence.\n","\n","<!-- * (Optional) The way I think of why we need different transformations in neural net: Just like playing Lego we need more than regular pieces🔺 to build real-world things 🚉, 📱... , neural network consist of more than linear transformation to model the real-world problem. This depends on prior knowledge of what problem you want to solve.  For example,  \n","if you build ⏰, you need irregularly shaped pieces like wheel🏽⚙️. \n","If you predict the increase of human population in China next year, you may use exponential function since you know  it will increase exponentially by time. \n","If you predict image class, you will use convolution transformation since yoou know image has to be understood by its combination of pixels rather than each pixel separately. -->"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"color:red\">I comment some codes for implementing different architectures here.</p>\n","<!-- ### 3.1 RNN\n","\n","More details see [RNN/LSTM](https://www.kaggle.com/sergioli212/rnn-lstm-from-scratch/)\n","\n","Code:\n","# input data with middle dimention 10 represents sequence length\n","X = torch.Tensor(np.random.randn(10, 3, 4))\n","n_x, m, T_x = X.shape # 3, 10, 4\n","n_y, n_a = 2, 5 \n","\n","# pytorch linear API + tanh activation function\n","linear_xa = torch.nn.Linear(n_x, n_a)\n","linear_aa = torch.nn.Linear(n_a, n_a)\n","a_prev = torch.randn(m, n_a)\n","for t in range(T_x):\n","    input1 = linear_xa(X[:,:,t])\n","    input2 = linear_aa(a_prev)\n","    a_prev = torch.tanh(input1+input2)\n","# print(a_prev.shape)\n","\n","\n","### 3.2 CNN\n","\n","# code\n","# keras\n","# tf.keras.layers.Conv1D(\n","#     filters,\n","#     kernel_size,\n","#     strides=1,\n","#     padding=\"valid\",\n","#     data_format=\"channels_last\",\n","#     dilation_rate=1,\n","#     groups=1,\n","#     activation=None,\n","#     use_bias=True,\n","#     kernel_initializer=\"glorot_uniform\",\n","#     bias_initializer=\"zeros\",\n","#     kernel_regularizer=None,\n","#     bias_regularizer=None,\n","#     activity_regularizer=None,\n","#     kernel_constraint=None,\n","#     bias_constraint=None,\n","#     **kwargs\n","# )\n","# The inputs are 128-length vectors with 10 timesteps, and the batch size is 4.  \n","input_shape = (4, 10, 128)\n","x = tf.random.normal(input_shape)\n","y = tf.keras.layers.Conv1D(32, 3, activation='relu',input_shape=input_shape[1:])(x)\n","print(y.shape)\n","\n","# pytorch: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n","# torch.nn.Conv1d(\n","#     in_channels,  # unlike keras, pytorch cannot infer the input channels since it has no placeholder for input\n","                    # put sequence length here\n","#     out_channels, \n","#     kernel_size, \n","#     stride=1,\n","#     padding=0, \n","#     dilation=1, \n","#     groups=1, \n","#     bias=True, \n","#     padding_mode='zeros')\n","x = tf.constant(\n","[[[1.,2.,3.],\n","  [2.,2.,1.],\n","  [3.,2.,6.]],\n","\n"," [[4.,4.,5.],\n","  [5.,5.,5.],\n","  [6.,6.,7.,],],\n","\n"," [[7.,8.,8.,],\n","  [8.,6.,7.,],\n","  [9.,8.,8.,]]] , dtype='float32')\n","print(x)\n","\n","# keras API\n","max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()\n","# max_pool_1d = tf.keras.layers.MaxPooling1D(3)\n","print(max_pool_1d(x))\n","\n","# jax API\n","from jax import random\n","import jax.numpy as jnp\n","key = random.PRNGKey(1701)\n","x = jnp.linspace(0, 10, 500)\n","y = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\n","window = jnp.ones(10) / 10\n","y_smooth = jnp.convolve(y, window, mode='same')\n","plt.plot(x, y, 'lightgray')\n","plt.plot(x, y_smooth, 'black'); -->"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-13T06:19:58.306836Z","iopub.status.busy":"2023-05-13T06:19:58.306127Z","iopub.status.idle":"2023-05-13T06:19:58.312114Z","shell.execute_reply":"2023-05-13T06:19:58.311094Z","shell.execute_reply.started":"2023-05-13T06:19:58.306793Z"}},"source":["\n","## 4. How to Acquire A Set of Weights Optimized for the Data/Task?\n","1. Randomly initialized weights\n","2. Define loss function. During optimization, it can can be used as an minimization objective function for weight optimization.\n","3. Iteratively update weights to minimize the loss function using gradient descent\n","\n","### 4.1 [Loss function](https://www.kaggle.com/sergioli212/cross-entropy-loss-details).\n","* Intuitively, it is used to quantify, given empirical data (i.e., training samples), how wrong the current model predicts in contrast to the true values of samples. The quantity is called the loss between model outputs and grouth truth.\n","* Let's design loss function for classification and regression problems?\n","  \n","* The formal definition relates to **Bayes Theorem** for machine learning in Week 2 and **Minimizing Negative Log Likelhood** in Week 6\n","    + Bayes Theorem: maximize Pr(parameters|data) $\\propto$ Pr(data|parameters) * Pr(parameters)\n","    + Likelihood: $ L(\\theta) =  Pr((X, y)|\\theta) $\n","    + Given a set of $\\theta$, the likelihood can be calculated by \"the product of the probabilities for each data point\". The likelihood measures whether a given $\\theta$ is a good estimation. \n","$ L(\\theta) =  Pr((X, y)|\\theta)= \\prod_{i=1}^{N} Pr\\left(y^{(i)}, x^{(i)} | \\theta\\right) =Pr\\left(y^{(i)} | x^{(i)} , \\theta\\right)$\n","    + Why does it call likelihood not probability? ⚠️ although we fix $\\theta$ to caluculate the probability $Pr(X, y | \\theta)$, we never know the only true $\\theta$ to calculate the true 👉$Pr(X, y | \\theta)$. The probability is really just fake. That's why we give it a new name --- `LIKELIHOOD`. \n","<!--     We normally use Negative Log Likelhood (NLL). -->\n","    + The optimal $\\theta$ should give the maximum liklihood or the minimum NLL $\\rightarrow$ We try our best to find a set of $\\theta$ which leads to NLL as low as possible, i.e., $ \\theta = argmax L(\\theta)=Pr(y | X , \\theta)$. \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["MLE for Binary Classification\n","* The last layer normaly has one neuron/probability from the sigmoid function, denoted as\n"," $ p=Pr(y = 1|x; \\theta)  $\n"," $$\\begin{aligned} NLL = -\\log L(\\theta) \\\\ &=-\\sum_{i=1}^{m} y^{(i)} \\log p+(1-y^{(i)}) \\log (1-p)) \\end{aligned}$$\n","\n","* It can be considered as **KL-devergence**, which measures the similarity between two distributions. [This post](https://jaketae.github.io/study/kl-mle/) proves that \"minimizing the KL divergence amounts to finding the maximum likelihood estimate of $\\theta$\". During the supervised learning，the ground-truth distribution is know. Hence, minizing **cross entropy** equals to the minimization of KL-devergence\n","$$D_{\\mathrm{KL}}(Y \\| \\hat{Y})=-\\sum_{i} y^{(\\mathrm{i})} \\log \\frac{\\hat{y}^{(\\mathrm{i})}}{y^{(\\mathrm{i})}}$$ "]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:28:40.964359Z","iopub.status.busy":"2023-05-17T11:28:40.963899Z","iopub.status.idle":"2023-05-17T11:28:40.973962Z","shell.execute_reply":"2023-05-17T11:28:40.972450Z","shell.execute_reply.started":"2023-05-17T11:28:40.964322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6597159476847235\n"]}],"source":["y_true = np.array([1])\n","log_probs = np.multiply(y_true ,np.log(y_prod)) + np.multiply((1-y_true), np.log(1-y_prod))\n","cost = (-1/len(y_true)) * np.sum(log_probs)\n","print(cost)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["MLE for Multiclass Classification\n","* Softmax Funtion: Except for sigmoid function, softmax function is used to calculate the output probability distribution for more than two classes\n","    $$p_i =\\frac{e^x_i}{\\sum_{i=1}^{K} e^{x_i}} $$"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:37:46.821297Z","iopub.status.busy":"2023-05-17T11:37:46.820848Z","iopub.status.idle":"2023-05-17T11:37:46.832336Z","shell.execute_reply":"2023-05-17T11:37:46.831144Z","shell.execute_reply.started":"2023-05-17T11:37:46.821258Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.84379473, 0.1141952 , 0.04201007],\n","       [0.00657326, 0.97555875, 0.01786798]])"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["def softmax(z):\n","    e_z = np.exp(z)\n","    return e_z / np.sum(e_z, axis=-1).reshape(-1, 1)\n","\n","logits = [[4.0, 2.0, 1.0], \n","          [0.0, 5.0, 1.0]]\n","\n","\n","y_prob = softmax(logits)\n","y_prob"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:37:06.370766Z","iopub.status.busy":"2023-05-17T11:37:06.370125Z","iopub.status.idle":"2023-05-17T11:37:06.386798Z","shell.execute_reply":"2023-05-17T11:37:06.385457Z","shell.execute_reply.started":"2023-05-17T11:37:06.370719Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(3.0973, dtype=torch.float64)\n","tensor(3.0973)\n"]}],"source":["# one-hot representations\n","labels = np.array([[1],[2]])\n","\n","# L(y_pred, y) = -sum(y * log y_pred for each class) = -log y_pred[true_class_idx]\n","total_loss = -np.log(np.take_along_axis(y_prob, labels, axis=-1))\n","np.mean(total_loss)\n","\n","# pytorch API\n","loss = F.nll_loss(torch.from_numpy(np.log(y_prob)), labels))\n","print(loss)\n","# loss = F.cross_entropy(torch.tensor(logits), torch.tensor([1, 2]))\n","\n","# Tensorflow API\n","# tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2. Optimization algorithm: [Gradient Descent](https://medium.com/@sergioli/understanding-gradient-descent-in-pytorch-dca50926bce4)\n","* Use gradients to update weights\n","* [Survey Paper from Sebastian Ruder](https://arxiv.org/abs/1609.04747)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calculating gradient via so-called backward propagation\n","* The `Tensor` class is the finest class we can trace the parameters and their corresponding gradients during backward propagation\n","* The attribute `requires_grad` to control whether the gradients should be calculated during the backward. Specifically, after performing operations (e.g., sin) on the input tensor during forward propagation, the `grad_fn` attribute would be added into the output tensor in order to calculate gradients afterward.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* Recording Tensor Operations: Computational Graph\n","[The official introduction](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph) \n","<!-- \n","> Conceptually, autograd keeps a record of data (tensors) & all executed\n","operations (along with the resulting new tensors) in a directed acyclic\n","graph (DAG) consisting of\n","[`Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n","objects. In this DAG, leaves are the input tensors, roots are the output\n","tensors. By tracing this graph from roots to leaves, you can\n","automatically compute the gradients using the chain rule.\n",">\n","> ![image.png](attachment:5e01a2e2-7506-448a-8e32-71531c5f6f12.png)![image.png](attachment:235839b6-fb87-45b4-a999-19d08bc69415.png)\n",">\n","> In a forward pass, autograd does two things simultaneously:\n","> \n","> - run the requested operation to compute a resulting tensor, and\n","> - maintain the operation’s *gradient function* in the DAG.\n",">\n","> The backward pass kicks off when ``.backward()`` is called on the DAG\n","root. ``autograd`` then:\n","> \n","> - computes the gradients from each ``.grad_fn``,\n","> - accumulates them in the respective tensor’s ``.grad`` attribute, and\n","> - using the chain rule, propagates all the way to the leaf tensors.\n","> \n","> Below is a visual representation of the DAG in our example. In the graph,\n","the arrows are in the direction of the forward pass. The nodes represent the backward functions\n","of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors ``a`` and ``b``.\n","> \n","> .. figure:: /_static/img/dag_autograd.png\n","> \n","> <div class=\"alert alert-info\"><h4>Note</h4><p>**DAGs are dynamic in PyTorch**\n","  An important thing to note is that the graph is recreated from scratch; after each\n","  ``.backward()`` call, autograd starts populating a new graph. This is\n","  exactly what allows you to use control flow statements in your model;\n","  you can change the shape, size and operations at every iteration if\n","  needed.</p></div>\n","> \n","> Exclusion from the DAG\n",">\n","> ``torch.autograd`` tracks operations on all tensors which have their\n","``requires_grad`` flag set to ``True``. For tensors that don’t require\n","gradients, setting this attribute to ``False`` excludes it from the\n","gradient computation DAG.\n",">\n","> The output tensor of an operation will require gradients even if only a\n","single input tensor has ``requires_grad=True``. -->"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:38:13.778719Z","iopub.status.busy":"2023-05-17T12:38:13.778323Z","iopub.status.idle":"2023-05-17T12:38:13.786490Z","shell.execute_reply":"2023-05-17T12:38:13.785411Z","shell.execute_reply.started":"2023-05-17T12:38:13.778686Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[ 93.3333,  43.6667, 113.3333,  ...,  60.3333,  41.6667,  82.6667],\n","        [106.0000,  41.0000, 132.6667,  ...,  79.6667,  41.0000,  83.6667],\n","        [118.0000,  38.0000, 143.6667,  ...,  96.6667,  51.3333,  81.3333],\n","        ...,\n","        [170.3333, 142.3333, 254.0000,  ..., 147.0000, 211.6667,  52.6667],\n","        [166.3333, 145.0000, 253.6667,  ..., 156.6667, 209.0000,  24.0000],\n","        [163.6667, 112.0000, 253.6667,  ..., 166.3333, 204.3333,  24.3333]])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["torch.from_numpy(X_batch.T)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:56:45.220227Z","iopub.status.busy":"2023-05-17T12:56:45.219808Z","iopub.status.idle":"2023-05-17T12:56:45.227728Z","shell.execute_reply":"2023-05-17T12:56:45.226423Z","shell.execute_reply.started":"2023-05-17T12:56:45.220188Z"},"trusted":true},"outputs":[],"source":["# create an input tensor\n","X_batch = X_train[:2]\n","y_batch = y_train[:2]\n","X_batch = torch.from_numpy(X_batch.T).float()\n","y_batch = torch.from_numpy(y_batch)\n","\n","\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:57:00.314943Z","iopub.status.busy":"2023-05-17T12:57:00.314520Z","iopub.status.idle":"2023-05-17T12:57:00.339878Z","shell.execute_reply":"2023-05-17T12:57:00.339100Z","shell.execute_reply.started":"2023-05-17T12:57:00.314909Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # This is added back by InteractiveShellApp.init_path()\n"]}],"source":["W1 = np.random.uniform(-1/np.sqrt(n_i), 1/np.sqrt(n_i), (n_m, n_i))\n","W1 = torch.from_numpy(W1).float()\n","W1.requires_grad=True\n","A1= torch.matmul(W1, X_batch)\n","A1 = F.tanh(A1)\n","\n","W2 = np.random.uniform(-1/np.sqrt(n_m), 1/np.sqrt(n_m), (5, n_m))\n","W2 =  torch.from_numpy(W2).float()\n","W2.requires_grad=True\n","A2= torch.matmul(W2, A1)\n","y_prob = F.softmax(A2.T)\n","\n","# input tensor has no grad_fn\n","# print(X_batch.grad_fn)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:57:03.029880Z","iopub.status.busy":"2023-05-17T12:57:03.029500Z","iopub.status.idle":"2023-05-17T12:57:03.036518Z","shell.execute_reply":"2023-05-17T12:57:03.035492Z","shell.execute_reply.started":"2023-05-17T12:57:03.029846Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([2, 3])"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["y_batch"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:57:04.725402Z","iopub.status.busy":"2023-05-17T12:57:04.724809Z","iopub.status.idle":"2023-05-17T12:57:04.732095Z","shell.execute_reply":"2023-05-17T12:57:04.730687Z","shell.execute_reply.started":"2023-05-17T12:57:04.725349Z"},"trusted":true},"outputs":[],"source":["loss = F.nll_loss(torch.log(y_prob), y_batch)\n"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:57:19.358387Z","iopub.status.busy":"2023-05-17T12:57:19.357979Z","iopub.status.idle":"2023-05-17T12:57:19.379707Z","shell.execute_reply":"2023-05-17T12:57:19.378828Z","shell.execute_reply.started":"2023-05-17T12:57:19.358352Z"},"trusted":true},"outputs":[],"source":["loss.backward()"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:57:24.283840Z","iopub.status.busy":"2023-05-17T12:57:24.283091Z","iopub.status.idle":"2023-05-17T12:57:24.293626Z","shell.execute_reply":"2023-05-17T12:57:24.292435Z","shell.execute_reply.started":"2023-05-17T12:57:24.283793Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[ 0.0134, -0.0008,  0.0145,  ..., -0.0002,  0.0173,  0.0156],\n","        [-0.0101, -0.0005, -0.0152,  ...,  0.0072,  0.0075,  0.0012],\n","        [-0.0043,  0.0013, -0.0150,  ...,  0.0117, -0.0130, -0.0144],\n","        ...,\n","        [-0.0001,  0.0130,  0.0130,  ...,  0.0055, -0.0174, -0.0105],\n","        [ 0.0017,  0.0101, -0.0002,  ...,  0.0109, -0.0180,  0.0169],\n","        [ 0.0055, -0.0080, -0.0150,  ...,  0.0163,  0.0152,  0.0036]],\n","       requires_grad=True)"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["W1"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["How a batch of samples （e.g., [x1, x2]） accumulate gradients? "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# parameters\n","w = torch.Tensor([1.,2.,3.])\n","print('Original w grad:', w.grad)\n","\n","\n","# we create `x` containing two examples, each has 3-dimensional features\n","x1 = torch.Tensor([1., 2., 3.])\n","x2 = torch.Tensor([4., 5., 6.])\n","\n","y = x1 * w\n","z = y.sum()\n","z.backward()\n","print('w grad with x1:', w.grad)\n","\n","y = x2 * w\n","z = y.sum()\n","w.grad = None\n","z.backward()\n","print('w grad with x2:', w.grad)\n","\n","\n","# The gradients will be summed up in the batch dimension\n","x = torch.Tensor([[1., 2., 3.],\n","                  [4., 5., 6.]])\n","y = x * w\n","z = y.sum()\n","w.grad = None\n","z.backward()\n","print('mini-batch gradient of parameters with x1 and x2:', w.grad)"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-12T05:53:44.937674Z","iopub.status.busy":"2023-05-12T05:53:44.937264Z","iopub.status.idle":"2023-05-12T05:53:44.946613Z","shell.execute_reply":"2023-05-12T05:53:44.945080Z","shell.execute_reply.started":"2023-05-12T05:53:44.937648Z"}},"source":["## 5. Deep Learning Frameworks\n","* The common transformations can be divided into two categories: \n","    + functions (no parameters during learning process):`pytorch.functional` contains the basic functions for high-level object-oriented modules with `torch.Tensor` (matrices) as parameters\n","    + architectures (containing learnable parameters): Rather than build neural network in a neuron/connection level, deep learning frameworks build NN in the **layer** level and use Object-oriented Implementations of Architectures. Two popular ones are Tensorflow `tensorflow.keras.layers` (e.g., `Conv1D`) and Pytorch `torch.nn.Module` (e.g., `Linear`, `Conv1d`, `Dropout`).\n","    + Pros/Cons of using layer-level architectures: only need to care inputs/outputs of each layers; unknown design decision\n","    + How to trace the parameters ?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<!-- Combination of Layers/Architectures\n","* how model parameters are used by `torch.nn.Module`?\n","* how they **nest** different layers or operations?\n","* How does backward propagation work in pytorch `AutoGrad`? -->"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T04:25:52.823912Z","iopub.status.busy":"2023-05-16T04:25:52.823433Z","iopub.status.idle":"2023-05-16T04:25:52.830071Z","shell.execute_reply":"2023-05-16T04:25:52.828889Z","shell.execute_reply.started":"2023-05-16T04:25:52.823877Z"},"trusted":true},"outputs":[],"source":["# # sklearn\n","# mlp = MLPClassifier(hidden_layer_sizes=(256,), \\\n","#                     activation=\"relu\", \\\n","#                     solver='adam',\\\n","#                     beta_1=0.9, beta_2=0.999, \\\n","#                     learning_rate='constant', learning_rate_init=0.001, \\\n","#                     max_iter=1000, random_state=0)\n","# for i in range(mlp.n_layers_)[:-1]:\n","#     print(i+1, '-th layer: ', mlp.coefs_[i].shape)\n","# mlp.fit(X_train, y_train)\n","# plt.plot(mlp.loss_curve_)\n","# plt.show()\n","# y_pred = mlp.predict(X_test)\n","# print(confusion_matrix(y_test, y_pred))\n","# print(classification_report(y_test, y_pred, target_names=target_names))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T04:35:54.653311Z","iopub.status.busy":"2023-05-16T04:35:54.652887Z","iopub.status.idle":"2023-05-16T04:35:54.691274Z","shell.execute_reply":"2023-05-16T04:35:54.690322Z","shell.execute_reply.started":"2023-05-16T04:35:54.653269Z"},"trusted":true},"outputs":[],"source":["# Create model\n","torch.manual_seed(0)\n","class MLP(torch.nn.Module):\n","    # a MLP model with 16 hidden layers\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(MLP, self).__init__()\n","        # 16 hidden layers\n","        for i in range(16):\n","            setattr(self, 'fc'+str(i), torch.nn.Linear(input_size, hidden_size))\n","            input_size = hidden_size\n","        self.fc_out = torch.nn.Linear(hidden_size, num_classes)\n","        self.relu = torch.nn.ReLU()\n","    \n","    def forward(self, x):\n","        out = x\n","        for i in range(16):\n","            out = getattr(self, 'fc'+str(i))(out)\n","            out = self.relu(out)\n","        out = self.fc_out(out)\n","        \n","        return out\n","\n","# mlp = MLP(h*w, 512, len(target_names))\n","mlp = torch.nn.Sequential(\n","    torch.nn.Linear(2914, 1024),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(1024, 512),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(512, 256),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(256, 5),\n",")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T04:35:56.205305Z","iopub.status.busy":"2023-05-16T04:35:56.204968Z","iopub.status.idle":"2023-05-16T04:36:19.948727Z","shell.execute_reply":"2023-05-16T04:36:19.946007Z","shell.execute_reply.started":"2023-05-16T04:35:56.205277Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:  0  Loss:  11.462784767150879\n","Epoch:  1  Loss:  126.77574920654297\n","Epoch:  2  Loss:  339.53106689453125\n","Epoch:  3  Loss:  201.82308959960938\n","Epoch:  4  Loss:  232.55670166015625\n","Epoch:  5  Loss:  124.06748962402344\n","Epoch:  6  Loss:  37.500946044921875\n","Epoch:  7  Loss:  22.166349411010742\n","Epoch:  8  Loss:  21.941390991210938\n","Epoch:  9  Loss:  40.41465377807617\n","Epoch:  10  Loss:  24.672550201416016\n","Epoch:  11  Loss:  34.74315643310547\n","Epoch:  12  Loss:  20.248130798339844\n","Epoch:  13  Loss:  18.979869842529297\n","Epoch:  14  Loss:  17.249162673950195\n","Epoch:  15  Loss:  18.310232162475586\n","Epoch:  16  Loss:  10.818763732910156\n","Epoch:  17  Loss:  12.157012939453125\n","Epoch:  18  Loss:  10.717308044433594\n","Epoch:  19  Loss:  11.436904907226562\n","Epoch:  20  Loss:  8.422748565673828\n","Epoch:  21  Loss:  6.02554178237915\n","Epoch:  22  Loss:  8.937173843383789\n","Epoch:  23  Loss:  6.57012414932251\n","Epoch:  24  Loss:  4.803252220153809\n","Epoch:  25  Loss:  5.202418327331543\n","Epoch:  26  Loss:  9.240605354309082\n","Epoch:  27  Loss:  7.206940174102783\n","Epoch:  28  Loss:  3.6451187133789062\n","Epoch:  29  Loss:  4.307581424713135\n","Epoch:  30  Loss:  4.019667148590088\n","Epoch:  31  Loss:  3.311680793762207\n","Epoch:  32  Loss:  4.134831428527832\n","Epoch:  33  Loss:  3.1524672508239746\n","Epoch:  34  Loss:  2.2832677364349365\n","Epoch:  35  Loss:  2.3392577171325684\n","Epoch:  36  Loss:  3.4613823890686035\n","Epoch:  37  Loss:  1.9950047731399536\n","Epoch:  38  Loss:  1.83784818649292\n","Epoch:  39  Loss:  2.358124256134033\n","Epoch:  40  Loss:  1.7169264554977417\n","Epoch:  41  Loss:  1.704728364944458\n","Epoch:  42  Loss:  1.903298258781433\n","Epoch:  43  Loss:  1.8005244731903076\n","Epoch:  44  Loss:  1.636776328086853\n","Epoch:  45  Loss:  1.5977765321731567\n","Epoch:  46  Loss:  1.6793206930160522\n","Epoch:  47  Loss:  1.5345426797866821\n","Epoch:  48  Loss:  1.5607056617736816\n","Epoch:  49  Loss:  1.5751229524612427\n","Epoch:  50  Loss:  1.5070874691009521\n","Epoch:  51  Loss:  1.4991694688796997\n","Epoch:  52  Loss:  1.5099714994430542\n","Epoch:  53  Loss:  1.5335779190063477\n","Epoch:  54  Loss:  1.4771543741226196\n","Epoch:  55  Loss:  1.450735092163086\n","Epoch:  56  Loss:  1.4761329889297485\n","Epoch:  57  Loss:  1.4584113359451294\n","Epoch:  58  Loss:  1.423580288887024\n","Epoch:  59  Loss:  1.4233239889144897\n","Epoch:  60  Loss:  1.4333032369613647\n","Epoch:  61  Loss:  1.4158257246017456\n","Epoch:  62  Loss:  1.3996726274490356\n","Epoch:  63  Loss:  1.4002549648284912\n","Epoch:  64  Loss:  1.4000494480133057\n","Epoch:  65  Loss:  1.384840488433838\n","Epoch:  66  Loss:  1.374527096748352\n","Epoch:  67  Loss:  1.37709641456604\n","Epoch:  68  Loss:  1.3687233924865723\n","Epoch:  69  Loss:  1.3551950454711914\n","Epoch:  70  Loss:  1.3527601957321167\n","Epoch:  71  Loss:  1.3499404191970825\n","Epoch:  72  Loss:  1.337654948234558\n","Epoch:  73  Loss:  1.3299686908721924\n","Epoch:  74  Loss:  1.3278961181640625\n","Epoch:  75  Loss:  1.3193845748901367\n","Epoch:  76  Loss:  1.310133695602417\n","Epoch:  77  Loss:  1.305696725845337\n","Epoch:  78  Loss:  1.2990851402282715\n","Epoch:  79  Loss:  1.2901543378829956\n","Epoch:  80  Loss:  1.2836394309997559\n","Epoch:  81  Loss:  1.2776808738708496\n","Epoch:  82  Loss:  1.2689974308013916\n","Epoch:  83  Loss:  1.261394739151001\n","Epoch:  84  Loss:  1.2552087306976318\n","Epoch:  85  Loss:  1.2466800212860107\n","Epoch:  86  Loss:  1.238513469696045\n","Epoch:  87  Loss:  1.2316845655441284\n","Epoch:  88  Loss:  1.2230125665664673\n","Epoch:  89  Loss:  1.2147595882415771\n","Epoch:  90  Loss:  1.2070374488830566\n","Epoch:  91  Loss:  1.1980879306793213\n","Epoch:  92  Loss:  1.1894810199737549\n","Epoch:  93  Loss:  1.1810139417648315\n","Epoch:  94  Loss:  1.1716811656951904\n","Epoch:  95  Loss:  1.1624234914779663\n","Epoch:  96  Loss:  1.1531198024749756\n","Epoch:  97  Loss:  1.1425706148147583\n","Epoch:  98  Loss:  1.1291943788528442\n","Epoch:  99  Loss:  1.1178392171859741\n","[[45  3 11  0  2]\n"," [ 3 29  4  0  0]\n"," [ 6 10 95  8  3]\n"," [ 0  1  4 20  6]\n"," [ 3  2  3  7 20]]\n","                   precision    recall  f1-score   support\n","\n","     Colin Powell       0.79      0.74      0.76        61\n","  Donald Rumsfeld       0.64      0.81      0.72        36\n","    George W Bush       0.81      0.78      0.79       122\n","Gerhard Schroeder       0.57      0.65      0.61        31\n","       Tony Blair       0.65      0.57      0.61        35\n","\n","         accuracy                           0.73       285\n","        macro avg       0.69      0.71      0.70       285\n","     weighted avg       0.74      0.73      0.73       285\n","\n"]}],"source":["# Train model\n","optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n","\n","# assign the class weights according to the number of samples\n","# in each class in X\n","class_weights = [X_train.shape[0]/np.sum(y_train==i) for i in range(len(target_names))]\n","# class_weights = [1.0, 1.0, 1.0, 1.0, 1.0]\n","class_weights = torch.tensor(class_weights, dtype=torch.float)\n","\n","\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","    outputs = mlp(X_train_tensor)\n","    loss = F.cross_entropy(outputs, y_train_tensor, weight=class_weights)\n","    loss.backward()\n","    optimizer.step()\n","    print('Epoch: ', epoch, ' Loss: ', loss.item())\n","\n","# Predict\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","y_pred = mlp(X_test_tensor).argmax(dim=1).numpy()\n","\n","# Evaluate\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred, target_names=target_names))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## High-level neural network using `nn.Module`\n","\n","As said above, a neural network is just a stack of operations on data input tensors and model parameter tensors. `nn.Module` has the basic implementation to record the model parameters and operations in high level.\n","\n","In a nutshell, \n","\n","**All the neural networks in Pytorch are built upon the parent class `nn.Module`**\n","\n","The following code cell demonstrates how model parameters are used by Linear module `class Linear(Module)`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-13T06:40:36.758152Z","iopub.status.busy":"2021-10-13T06:40:36.757776Z","iopub.status.idle":"2021-10-13T06:40:36.767176Z","shell.execute_reply":"2021-10-13T06:40:36.765828Z","shell.execute_reply.started":"2021-10-13T06:40:36.758121Z"},"trusted":true},"outputs":[],"source":["# Use Pytorch Linear Module\n","nn_module = nn.Linear(5, 2)\n","for p in nn_module.parameters():\n","    print('W or b: ', p.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Module` is used **in a nested way**."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-22T07:57:52.806999Z","iopub.status.busy":"2021-05-22T07:57:52.806615Z","iopub.status.idle":"2021-05-22T07:57:52.819659Z","shell.execute_reply":"2021-05-22T07:57:52.818732Z","shell.execute_reply.started":"2021-05-22T07:57:52.806968Z"},"trusted":true},"outputs":[],"source":["# build customized pytorch nn modules\n","class Network(nn.Module):\n","    def __init__(self):\n","        super().__init__() # pytorch will register layers and operations we put into the network\n","        \n","        self.hidden = nn.Linear(784, 256)\n","        self.output = nn.Linear(256, 10)\n","        \n","        self.sigmoid = nn.Sigmoid()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        x = self.hidden(x)\n","        x = self.sigmoid(x)\n","        x = self.output(x)\n","        x = self.softmax(x)\n","        \n","        return x\n","        \n","model = Network()\n","model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Closing Words \n","* A neural network is an universal function approximator. For example, if you use softmax or sigmoid, a neural network represents conditional probability distribution. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n","* the above models/functions are not trained so they are not a model fitting into any data. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n","* Bullshit: The reason I would like to decompose NN architecture and modelling process is because, I think, I always see a lot of implementation packages and tutorials tend to mix up concepts or algorithms as a black box which hurts the flexibility to identify  and adjust finer pieces for solving my problem. For example, to generate adversarial text of NLP, I had a trouble to understand why we could update input instead of model parameters and what is problem of discrete text space and how to add constraints for ensuring fluency fo generating text via loss function (or adversarial objective functions here). All in all, each functions (linear, activation, stacking function, loss functions) during forward pass and their differentiation properties during backward pass should be well understood for deep learning practitioners.\n","\n","## Reference\n","* [How to initialize weights?](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
