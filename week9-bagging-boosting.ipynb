{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.66\n",
      "The test error rate is 0.34\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(N = 30):\n",
    "    # generate a sample of size N = 30, with two classes and 5 features \n",
    "    # each having a standard Gaussian distribution with pairwise correlation 0.95\n",
    "    num_features = 5\n",
    "    corr_matrix = np.ones((num_features, num_features)) * 0.95 + np.identity(num_features) * 0.05\n",
    "    X = np.random.multivariate_normal(mean=np.zeros(num_features), cov=corr_matrix, size=N)\n",
    "\n",
    "    # The response Y was generated according to Pr(Y = 1|x1 ≤ 0.5) = 0.2, Pr(Y = 1|x1 > 0.5) = 0.8\n",
    "    # what is the lowest possible error rate?\n",
    "    Y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if X[i,0] <= 0.5: # 80% to be 0\n",
    "            Y[i] = np.random.binomial(1, 0.2)\n",
    "        else: # 80% to be 1\n",
    "            Y[i] = np.random.binomial(1, 0.8) \n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X_train, y_train = generate_sample(N = 40) # we need some variabiliy in the data for bootstrap to work\n",
    "X_test, y_test = generate_sample(N = 2000)\n",
    "\n",
    "treeclf = DecisionTreeClassifier(random_state=1)\n",
    "treeclf.fit(X_train, y_train)\n",
    "y_train_pred = treeclf.predict(X_train)\n",
    "y_pred = treeclf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('The test error rate is {:.2f}'.format(1 - accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an Ensemble model: train different independent models on slightly different subsets of data\n",
    "* How to make each model independent with others? \n",
    "* Hint: The way the data is fed into the models can be challenging\n",
    "\n",
    "## Bagging\n",
    "* Trained Multiple Models on Bootstrap datasets\n",
    "    + Bootstrap: Resampling the same size of sample with replacement; reduce variance\n",
    "    + Bagging (or Bootstrap aggregation): agggregate the prediction over a collection of bootstrap samples\n",
    "    + A Bootstrap sample $\\mathbf{Z}^{* b}, b=1,2, \\ldots, B$ -> a fitted model $\\hat{f}^{* b}(x)$\n",
    "        $$\\hat{f}_{\\mathrm{bag}}(x)=\\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{* b}(x)$$\n",
    "\n",
    "Bagging Inference\n",
    "* Voting for classification: $\\hat{G}_{\\text {bag }}(x)=\\arg \\max _k \\hat{f}_{\\mathrm{bag}}(x)$\n",
    "    + \"It is tempting to treat the voting proportions pk(x) as estimates of these probabilities. A simple two-class example shows that they fail in this regard.\" (Hastie, 2008) **How/why fails?**\n",
    "     <!--  Suppose the true probability of class 1 at x is 0.75, and each of the bagged classifiers accurately predict a 1. Then p1(x) = 1, which is incorrect. -->\n",
    "    + \"An alternative bagging strategy is to average these instead, rather than the vote indicator vectors.\"\n",
    "* Averaging for regression \n",
    "* Out-of-bag samples: about 1/3 original data is not in the bootstrap dataset which can be used for model evaluation\n",
    "\n",
    "Goal: reduce the variance of unstable (high variance) learning methods. Assuming that the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation ρ, the variance\n",
    "of the average is\n",
    "\\begin{equation}\n",
    "\\rho \\sigma^2+\\frac{1-\\rho}{B} \\sigma^2\n",
    "(\\#eq:variance)\n",
    "\\end{equation}\n",
    "\n",
    "<!-- Question 1: Which learning model/method is ideal for bagging?\n",
    "\n",
    "Question 2: Will it reduce bias?\n",
    "\n",
    "\"since each tree generated in bagging is [identically distributed (i.d.)](https://stats.stackexchange.com/questions/89036/why-the-trees-generated-via-bagging-are-identically-distributed#:~:text=Bagging%20technique%20uses%20bootstraps%20\\(random,population%20as%20the%20original%20sample.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d.\" (Hastie, 2008) -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.70\n",
      "The test error rate is 0.30\n"
     ]
    }
   ],
   "source": [
    "# bootstrap sample\n",
    "def boostrap(X, y):\n",
    "    X_boot = []\n",
    "    y_boot = []\n",
    "    for i in range(X.shape[0]):\n",
    "        idx = np.random.randint(0, len(X_train))\n",
    "        X_boot.append(X[idx])\n",
    "        y_boot.append(y[idx])\n",
    "    return X_boot, y_boot\n",
    "\n",
    "# generate 200 bootstrap samples\n",
    "bootstrap_samples = []\n",
    "bootstrap_labels = []\n",
    "for _ in range(2000):\n",
    "    X_boot, y_boot = boostrap(X_train, y_train)\n",
    "    bootstrap_samples.append(X_boot)\n",
    "    bootstrap_labels.append(y_boot)\n",
    "\n",
    "\n",
    "# fit an ensemble of classification trees\n",
    "ensemble_clf = []\n",
    "for i in range(2000):\n",
    "    ensemble_clf.append(DecisionTreeClassifier(random_state=1, max_features=2).fit(bootstrap_samples[i], bootstrap_labels[i]))\n",
    "\n",
    "# predict the test data\n",
    "y_preds = []\n",
    "for clf in ensemble_clf:\n",
    "    y_preds.append(clf.predict(X_test))\n",
    "y_pred = (np.array(y_preds).mean(axis=0) > 0.5).astype(int)\n",
    "\n",
    "y_train_preds = []\n",
    "for clf in ensemble_clf:\n",
    "    y_train_preds.append(clf.predict(X_train))\n",
    "y_train_pred = (np.array(y_train_preds).mean(axis=0) > 0.5).astype(int)\n",
    "\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('The test error rate is {:.2f}'.format(1 - accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/bagging.png\" width=\"500\"></center>\n",
    "\n",
    "<center><img src=\"pics/bagging_result.png\" width=\"500\"></center>\n",
    "\n",
    "* the trees have high variance due to the correlation in the predictors\n",
    "* Bagging succeeds in smoothing out this variance and hence reducing the test error\n",
    "    + \" averaging reduces variance and leaves bias unchanged\" (Hastie, 2008)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324)\n",
    "* \"the size of the correlation of pairs of bagged trees limits the benefits of averaging\" according to Formula 1\n",
    "* \"a substantial modification of bagging that builds a large collection of de-correlated trees\"\n",
    "* Iteratively 1) make a bootstrapped dataset; 2) only use a random subset of variables at each splitting (`max_features`)\n",
    "* can handle large data sets with higher dimensionality (thousands of input variables).\n",
    "* can identify most significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.69\n",
      "The test error rate is 0.31\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 200 \n",
    "rf_clf = RandomForestClassifier(n_estimators=n_estimators, criterion=\"gini\", max_features=None) #  max_depth=5,\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(rf_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(rf_clf.score(X_test, y_test)))\n",
    "print('The test error rate is {:.2f}'.format(1 - rf_clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# for estimator in rf_clf.estimators_:\n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     tree.plot_tree(estimator, filled=True, rounded=True)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Looking for good `n_estimators`](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "What to boost?\n",
    "\n",
    "* AdaBoost\n",
    "    + an fix-sized estimator uses only one feature, i.e., one stump (one root node with two leaf nodes)\n",
    "    + weak learner\n",
    "    + build the subsequent stumps using the residuals\n",
    "    + the amount of say \n",
    "        $$\\alpha_m=\\log \\left(\\left(1-\\operatorname{err}_m\\right) / \\operatorname{err}_m\\right)$$\n",
    "\n",
    "    + Update the weights\n",
    "        $$w_i \\cdot \\exp \\left[\\alpha_m \\cdot I\\left(y_i \\neq G_m\\left(x_i\\right)\\right)\\right], i=1,2, \\ldots, N$$\n",
    "<!-- Emphasize the need to correctly classify the examples with wrong predictions in the previous steps -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.69\n",
      "The test error rate is 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73      1197\n",
      "         1.0       0.61      0.66      0.63       803\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.68      0.69      0.68      2000\n",
      "weighted avg       0.70      0.69      0.69      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build an Adaboost classifier with 200 trees\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(ada_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(ada_clf.score(X_test, y_test)))\n",
    "print('The test error rate is {:.2f}'.format(1 - ada_clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* GradientBoost\n",
    "    + an fixeds-size estimator normally has 8 to 32 leaves\n",
    "    + iteratively fit residuals by a split\n",
    "    + use learning rate to avoid high bias\n",
    "    + [Youtube Course](vhttps://www.youtube.com/watch?v=3CC4N4z3GJc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on test data is 0.13\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('data/sample_dataset.csv')\n",
    "\n",
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "target_col = train_data.columns[-1]\n",
    "X_train = train_data.drop(target_col, axis=1)\n",
    "X_train = X_train.iloc[:, 2:] # remove the first two columns: subject ID and sample ID\n",
    "X_test = test_data.drop(target_col, axis=1)\n",
    "X_test = X_test.iloc[:, 2:]\n",
    "y_train = train_data[target_col]\n",
    "y_test = test_data[target_col]\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('The mean squared error on test data is {:.2f}'.format(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on test data is 0.13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('The mean squared error on test data is {:.2f}'.format(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "        \n",
    "            # Fit a decision tree to the negative gradient of the loss function\n",
    "            residual = y_test - y_pred\n",
    "            fit(X, residual)\n",
    "            # Add the new tree to the ensemble\n",
    "         \n",
    "\n",
    "            # calculate current residuals\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions to be self.init_pred\n",
    "  \n",
    "        # Iterate over each decision tree in the ensemble and make predictions\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/gradient-boost1.png\" width=\"500\"></center>\n",
    "\n",
    "<center><img src=\"pics/gradient-boost2.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.69\n",
      "The test error rate is 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73      1197\n",
      "         1.0       0.60      0.66      0.63       803\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a Gradient Boosting classifier with 200 trees\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=1, random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('The test error rate is {:.2f}'.format(1 - gb_clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read titanic data\n",
    "# titanic_df = pd.read_csv('data/titanic_cleaned_data.csv')\n",
    "# print(titanic_df.columns)\n",
    "# # define X and y\n",
    "# # Port of Embarkation: Q = Queenstown, S = Southampton\n",
    "# feature_cols = ['Pclass', 'Sex', 'Embarked_Q', 'Embarked_S', 'Age', 'PassengerId']\n",
    "# X = titanic_df[feature_cols]\n",
    "# y = titanic_df.Survived\n",
    "# X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Finding Important Features\n",
    "# importances = rf_clf.feature_importances_\n",
    "# print(importances)\n",
    "# forest_importances = pd.Series(importances, index=list(X_train.columns))\n",
    "# std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\n",
    "# print(forest_importances.head())\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 8 features will be used: ['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime'].\n",
      "The values for Attrition column are: \n",
      "No     1233\n",
      "Yes     237\n",
      "Name: Attrition, dtype: int64\n",
      "The Attrition column has been encoded. The values are: \n",
      "0    1233\n",
      "1     237\n",
      "Name: Attrition, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.75      0.78      1233\n",
      "         1.0       0.64      0.70      0.67       767\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.72      0.73      0.72      2000\n",
      "weighted avg       0.74      0.73      0.73      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        24\n",
      "         1.0       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1: Load HR-Employee-Attrition.csv dataset and \n",
    "# create an ensemble ML model for predicting target variable (Attrition). \n",
    "# Report the performance of the model using appropriate metrics.\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"data/HR-Employee-Attrition.csv\")\n",
    "    df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)\n",
    "    categorical_col = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object and len(df[column].unique()) <= 50:\n",
    "            categorical_col.append(column)\n",
    "    print(f'The {len(categorical_col)} features will be used: {categorical_col}.')\n",
    "    print('The values for Attrition column are: ')\n",
    "    print(df.Attrition.value_counts())\n",
    "    df['Attrition'] = df.Attrition.astype(\"category\").cat.codes\n",
    "    print('The Attrition column has been encoded. The values are: ')\n",
    "    print(df.Attrition.value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(classification_report(y_train, rf_clf.predict(X_train)))\n",
    "\n",
    "# Q2: Have you used any hyperparameter tuning while building the model in Q1? \n",
    "# If so then plot your performance metrics for different hyperparmeter values that you have used in Q1.  Hints\n",
    "\n",
    "# Q3: Reflect on the importance of hyperparameter tuning of ML models based on your ML model development exercise. \n",
    "\n",
    "# 4. Create a GradientBoost model for the predicting Attrition using the same dataset that you have used Q1 and report the performance.\n",
    "\n",
    "# 5. Compare the performance of two models (Q1 and Q3). Explain which model is good and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sit720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
